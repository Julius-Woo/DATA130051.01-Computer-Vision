{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Fine-tuning a CNN Pretrained on ImageNet for Bird Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelimiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set hyperparameters\n",
    "data_dir = 'CUB_200_2011'\n",
    "num_classes = 200\n",
    "batch_size = 64\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "l2 = 0.01\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load data\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=8) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, dataloaders, dataset_sizes, learning_rate, num_epochs, l2, patience, param_group=True):\n",
    "    \"\"\"\n",
    "    Fine-tunes a pretrained model on a new dataset.\n",
    "    \n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network to train.\n",
    "        dataloaders (dict): A dictionary containing the training and validation dataloaders.\n",
    "        dataset_sizes (dict): A dictionary containing the sizes of the training and validation datasets.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        num_epochs (int, optional): The number of epochs to train for.\n",
    "        l2: Weight decay strength.\n",
    "        patience: The patience number for early stopping.\n",
    "        param_group (bool, optional): If True, use a higher learning rate for the final layer. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "        net (torch.nn.Module): The fine-tuned neural network.\n",
    "    \"\"\"\n",
    "    log_dir = os.path.join(\"runs/CUB_bird_classification\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Define the optimizer with parameter groups if param_group is True\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters() if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        optimizer = optim.SGD([\n",
    "            {'params': params_1x},\n",
    "            {'params': net.fc.parameters(), 'lr': learning_rate * 10}\n",
    "        ], lr=learning_rate, weight_decay=l2, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=l2, momentum=momentum)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.05)\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())  # Save the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)  # Forward pass\n",
    "                    _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "                    loss = loss_fn(outputs, labels)  # Compute loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # Backward pass\n",
    "                        optimizer.step()  # Optimize the model\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # TensorBoard\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Training loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Training accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('Validation loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # Deep copy the model if the validation accuracy is the best seen so far\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                # Early stopping\n",
    "                if epochs_no_improve == patience:\n",
    "                    print('Early stopping!')\n",
    "                    net.load_state_dict(best_model_wts)\n",
    "                    writer.close()\n",
    "                    torch.save(net.state_dict(), 'CUB_best_weights_ft.pth')\n",
    "                    return net\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    torch.save(net.state_dict(), 'CUB_best_weights_ft.pth')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning_search(net, dataloaders, dataset_sizes, lr_small, lr_large, num_epochs, l2, param_group=True):\n",
    "    \"\"\"\n",
    "    Function for grid search, verbose outputs omitted.\n",
    "    \"\"\"\n",
    "    log_dir = os.path.join(\"runs/CUB_bird_search\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define the optimizer with parameter groups if param_group is True\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters() if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        optimizer = optim.SGD([\n",
    "            {'params': params_1x},\n",
    "            {'params': net.fc.parameters(), 'lr': lr_large}\n",
    "        ], lr=lr_small, weight_decay=l2, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr_small, weight_decay=l2, momentum=momentum)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())  # Save the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)  # Forward pass\n",
    "                    _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "                    loss = loss_fn(outputs, labels)  # Compute loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # Backward pass\n",
    "                        optimizer.step()  # Optimize the model\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            if epoch == num_epochs - 1:\n",
    "                final_loss = epoch_loss\n",
    "                final_acc = epoch_acc\n",
    "                print(f'Final {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # TensorBoard\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Training loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Training accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('Validation loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # Deep copy the model if the validation accuracy is the best seen so far\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return net, final_loss, final_acc, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_scratch(net, dataloaders, dataset_sizes, optimizer, num_epochs):\n",
    "    \"\"\"Function for training a model from scratch.\n",
    "    \"\"\"\n",
    "    log_dir = os.path.join(\"runs/CUB_from_scratch\",\n",
    "                           datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('training loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('training accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('validation loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('validation accuracy', epoch_acc, epoch)\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    torch.save(net.state_dict(), 'CUB_best_weights_scratch.pth')\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloaders, criterion, phase='test'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    loss = running_loss / dataset_sizes[phase]\n",
    "    acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "    # print(f'{phase} Loss: {loss:.4f} Acc: {acc:.4f}')\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best parameters\n",
    "results = []\n",
    "\n",
    "def search_param(lrs, epochs, l2_s):\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for l2 in l2_s:\n",
    "                print(f'----Training with lr={lr}, epochs={epoch}, l2={l2}----')\n",
    "                net = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "                net.fc = nn.Linear(net.fc.in_features, 200)\n",
    "                net = net.to(device)\n",
    "\n",
    "                trained_net, final_loss, final_acc, best_acc = train_fine_tuning_search(\n",
    "                    net, dataloaders, dataset_sizes, lr[0], lr[1], epoch, l2)\n",
    "\n",
    "                test_loss, test_acc = evaluate_model(trained_net, dataloaders, nn.CrossEntropyLoss())\n",
    "\n",
    "                results.append((lr, epoch, l2, final_loss, final_acc, test_loss, test_acc, best_acc))\n",
    "                print(f'Test loss: {test_loss:.4f} Acc: {test_acc:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training with lr=(0.01, 0.1), epochs=15, l2=0.01----\n",
      "Final train Loss: 2.1697 Acc: 0.5078\n",
      "Final val Loss: 2.4523 Acc: 0.4178\n",
      "Best val Acc: 0.4904\n",
      "Test loss: 1.9993 Acc: 0.5074\n",
      "\n",
      "----Training with lr=(0.01, 0.01), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.9356 Acc: 0.5969\n",
      "Final val Loss: 2.1083 Acc: 0.4879\n",
      "Best val Acc: 0.5596\n",
      "Test loss: 1.7709 Acc: 0.5690\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.2869 Acc: 0.7358\n",
      "Final val Loss: 1.2314 Acc: 0.7048\n",
      "Best val Acc: 0.7106\n",
      "Test loss: 1.1907 Acc: 0.7164\n",
      "\n",
      "----Training with lr=(0.001, 0.001), epochs=15, l2=0.01----\n",
      "Final train Loss: 2.3232 Acc: 0.5994\n",
      "Final val Loss: 2.0525 Acc: 0.5997\n",
      "Best val Acc: 0.5997\n",
      "Test loss: 1.9915 Acc: 0.6134\n",
      "\n",
      "----Training with lr=(0.0005, 0.005), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.5252 Acc: 0.6970\n",
      "Final val Loss: 1.4174 Acc: 0.6747\n",
      "Best val Acc: 0.6747\n",
      "Test loss: 1.3381 Acc: 0.6964\n",
      "\n",
      "----Training with lr=(0.0001, 0.001), epochs=15, l2=0.01----\n",
      "Final train Loss: 3.1223 Acc: 0.4655\n",
      "Final val Loss: 2.8362 Acc: 0.4762\n",
      "Best val Acc: 0.4762\n",
      "Test loss: 2.7930 Acc: 0.4926\n",
      "\n",
      "----Training with lr=(1e-05, 0.0001), epochs=15, l2=0.01----\n",
      "Final train Loss: 5.0962 Acc: 0.0265\n",
      "Final val Loss: 5.0743 Acc: 0.0242\n",
      "Best val Acc: 0.0250\n",
      "Test loss: 5.1137 Acc: 0.0250\n",
      "\n",
      "----Training with lr=(5e-05, 0.0005), epochs=15, l2=0.01----\n",
      "Final train Loss: 4.1048 Acc: 0.2776\n",
      "Final val Loss: 3.9321 Acc: 0.2702\n",
      "Best val Acc: 0.2702\n",
      "Test loss: 3.9168 Acc: 0.2922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for lrs\n",
    "# (small, large)\n",
    "lrs = [(0.01, 0.1), (0.01, 0.01), (0.001, 0.01), (0.001, 0.001), (5e-4, 5e-3), \n",
    "       (1e-4, 1e-3), (1e-5, 1e-4), (5e-5, 5e-4)]\n",
    "epoch = 15\n",
    "\n",
    "search_param(lrs, [epoch], [0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training with lr=(0.001, 0.01), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.2756 Acc: 0.7426\n",
      "Final val Loss: 1.2268 Acc: 0.7056\n",
      "Best val Acc: 0.7056\n",
      "Test loss: 1.1648 Acc: 0.7194\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.01----\n",
      "Final train Loss: 1.0975 Acc: 0.8017\n",
      "Final val Loss: 1.1870 Acc: 0.7248\n",
      "Best val Acc: 0.7248\n",
      "Test loss: 1.1144 Acc: 0.7347\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=45, l2=0.01----\n",
      "Final train Loss: 0.9513 Acc: 0.8367\n",
      "Final val Loss: 1.1712 Acc: 0.7206\n",
      "Best val Acc: 0.7381\n",
      "Test loss: 1.1074 Acc: 0.7453\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=65, l2=0.01----\n",
      "Final train Loss: 0.8487 Acc: 0.8659\n",
      "Final val Loss: 1.1642 Acc: 0.7323\n",
      "Best val Acc: 0.7456\n",
      "Test loss: 1.0980 Acc: 0.7503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for epochs\n",
    "lr = (0.001, 0.01)\n",
    "epochs = [15, 25, 45, 65]\n",
    "l2 = 0.01\n",
    "search_param([lr], epochs, [l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.1----\n",
      "Final train Loss: 4.4820 Acc: 0.1007\n",
      "Final val Loss: 4.3680 Acc: 0.1243\n",
      "Best val Acc: 0.3770\n",
      "Test loss: 3.2471 Acc: 0.3907\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.01----\n",
      "Final train Loss: 1.1049 Acc: 0.7931\n",
      "Final val Loss: 1.1633 Acc: 0.7189\n",
      "Best val Acc: 0.7256\n",
      "Test loss: 1.1167 Acc: 0.7301\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.001----\n",
      "Final train Loss: 0.6861 Acc: 0.8482\n",
      "Final val Loss: 1.0361 Acc: 0.7198\n",
      "Best val Acc: 0.7248\n",
      "Test loss: 0.9837 Acc: 0.7275\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.0001----\n",
      "Final train Loss: 0.6500 Acc: 0.8515\n",
      "Final val Loss: 1.0420 Acc: 0.7198\n",
      "Best val Acc: 0.7206\n",
      "Test loss: 0.9797 Acc: 0.7287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for l2 regularization strengths\n",
    "l2_s = [0.1, 0.01, 0.001, 0.0001]\n",
    "epoch = 25\n",
    "lr = (0.001, 0.01)\n",
    "search_param([lr], [epoch], l2_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 5.5233 Acc: 0.0156\n",
      "val Loss: 4.7857 Acc: 0.0601\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 4.3770 Acc: 0.1255\n",
      "val Loss: 3.7077 Acc: 0.1977\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 3.5087 Acc: 0.2609\n",
      "val Loss: 2.9357 Acc: 0.3403\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 2.8977 Acc: 0.3973\n",
      "val Loss: 2.4918 Acc: 0.4170\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.4901 Acc: 0.4792\n",
      "val Loss: 2.2088 Acc: 0.5071\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 2.2320 Acc: 0.5437\n",
      "val Loss: 2.0041 Acc: 0.5388\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.0737 Acc: 0.5704\n",
      "val Loss: 1.8437 Acc: 0.5680\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.9085 Acc: 0.6148\n",
      "val Loss: 1.7391 Acc: 0.5955\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.8279 Acc: 0.6265\n",
      "val Loss: 1.6664 Acc: 0.6155\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.7025 Acc: 0.6542\n",
      "val Loss: 1.6126 Acc: 0.6163\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.6307 Acc: 0.6753\n",
      "val Loss: 1.5453 Acc: 0.6480\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.5866 Acc: 0.6792\n",
      "val Loss: 1.4966 Acc: 0.6497\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.5517 Acc: 0.6872\n",
      "val Loss: 1.4714 Acc: 0.6539\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.4666 Acc: 0.7034\n",
      "val Loss: 1.4355 Acc: 0.6739\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.4578 Acc: 0.7016\n",
      "val Loss: 1.4111 Acc: 0.6606\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.4015 Acc: 0.7358\n",
      "val Loss: 1.3822 Acc: 0.6706\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.3731 Acc: 0.7314\n",
      "val Loss: 1.3731 Acc: 0.6697\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.3685 Acc: 0.7391\n",
      "val Loss: 1.3506 Acc: 0.6814\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.3125 Acc: 0.7460\n",
      "val Loss: 1.3344 Acc: 0.6906\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.3063 Acc: 0.7547\n",
      "val Loss: 1.3174 Acc: 0.6922\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2532 Acc: 0.7662\n",
      "val Loss: 1.2982 Acc: 0.6872\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2769 Acc: 0.7502\n",
      "val Loss: 1.2914 Acc: 0.6922\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2421 Acc: 0.7693\n",
      "val Loss: 1.2866 Acc: 0.6964\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2077 Acc: 0.7712\n",
      "val Loss: 1.2791 Acc: 0.6922\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2242 Acc: 0.7708\n",
      "val Loss: 1.2771 Acc: 0.6914\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.1867 Acc: 0.7831\n",
      "val Loss: 1.2607 Acc: 0.7023\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.1980 Acc: 0.7789\n",
      "val Loss: 1.2630 Acc: 0.7056\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.1748 Acc: 0.7864\n",
      "val Loss: 1.2519 Acc: 0.7114\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.1435 Acc: 0.7879\n",
      "val Loss: 1.2461 Acc: 0.7048\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.1749 Acc: 0.7798\n",
      "val Loss: 1.2299 Acc: 0.7123\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.1243 Acc: 0.7979\n",
      "val Loss: 1.2231 Acc: 0.7156\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.1134 Acc: 0.7987\n",
      "val Loss: 1.2406 Acc: 0.7064\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.1329 Acc: 0.7927\n",
      "val Loss: 1.2294 Acc: 0.7206\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.1083 Acc: 0.8033\n",
      "val Loss: 1.2265 Acc: 0.7189\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.1123 Acc: 0.8013\n",
      "val Loss: 1.2200 Acc: 0.7148\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.0792 Acc: 0.8092\n",
      "val Loss: 1.2156 Acc: 0.7106\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.0712 Acc: 0.8102\n",
      "val Loss: 1.2052 Acc: 0.7339\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.0857 Acc: 0.8117\n",
      "val Loss: 1.2041 Acc: 0.7114\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.0767 Acc: 0.8052\n",
      "val Loss: 1.1960 Acc: 0.7223\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.0634 Acc: 0.8092\n",
      "val Loss: 1.2061 Acc: 0.7181\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.0535 Acc: 0.8138\n",
      "val Loss: 1.1956 Acc: 0.7248\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.0495 Acc: 0.8192\n",
      "val Loss: 1.2032 Acc: 0.7248\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet-18 model\n",
    "model_ft = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "# Modify the fully connected layer to match the number of output classes\n",
    "model_ft.fc = nn.Linear(model_ft.fc.in_features, num_classes)\n",
    "# Initialize the weights of FC layer using Xavier uniform initialization\n",
    "nn.init.xavier_uniform_(model_ft.fc.weight)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "model_ft = train_fine_tuning(model_ft, dataloaders, dataset_sizes, learning_rate, num_epochs, l2, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 5.3608 Acc: 0.0083\n",
      "val Loss: 5.1666 Acc: 0.0200\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 5.0445 Acc: 0.0223\n",
      "val Loss: 4.8936 Acc: 0.0267\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 4.7982 Acc: 0.0321\n",
      "val Loss: 4.9673 Acc: 0.0267\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 4.6457 Acc: 0.0400\n",
      "val Loss: 4.5881 Acc: 0.0475\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 4.5421 Acc: 0.0509\n",
      "val Loss: 4.6949 Acc: 0.0475\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 4.4472 Acc: 0.0603\n",
      "val Loss: 4.5775 Acc: 0.0550\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 4.3055 Acc: 0.0713\n",
      "val Loss: 4.6730 Acc: 0.0559\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 4.2129 Acc: 0.0884\n",
      "val Loss: 4.2846 Acc: 0.0751\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 4.1206 Acc: 0.0966\n",
      "val Loss: 4.3907 Acc: 0.0801\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 4.0465 Acc: 0.1122\n",
      "val Loss: 4.2490 Acc: 0.0801\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 3.9810 Acc: 0.1164\n",
      "val Loss: 4.4095 Acc: 0.0826\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 3.8921 Acc: 0.1226\n",
      "val Loss: 4.4009 Acc: 0.0934\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 3.8043 Acc: 0.1404\n",
      "val Loss: 4.2358 Acc: 0.0909\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 3.7444 Acc: 0.1433\n",
      "val Loss: 4.2334 Acc: 0.0909\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 3.6702 Acc: 0.1550\n",
      "val Loss: 3.9313 Acc: 0.1243\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 3.5741 Acc: 0.1737\n",
      "val Loss: 3.9335 Acc: 0.1143\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 3.5166 Acc: 0.1773\n",
      "val Loss: 3.9557 Acc: 0.1318\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 3.4343 Acc: 0.1892\n",
      "val Loss: 3.9414 Acc: 0.1343\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 3.3293 Acc: 0.2123\n",
      "val Loss: 4.0948 Acc: 0.1435\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 3.2708 Acc: 0.2292\n",
      "val Loss: 3.9954 Acc: 0.1376\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 3.2230 Acc: 0.2319\n",
      "val Loss: 3.6515 Acc: 0.1660\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 3.1208 Acc: 0.2544\n",
      "val Loss: 4.0218 Acc: 0.1468\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 3.1015 Acc: 0.2507\n",
      "val Loss: 3.7116 Acc: 0.1551\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2.9920 Acc: 0.2799\n",
      "val Loss: 3.7538 Acc: 0.1793\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2.9222 Acc: 0.2909\n",
      "val Loss: 3.8112 Acc: 0.1935\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2.5907 Acc: 0.3664\n",
      "val Loss: 3.1677 Acc: 0.2644\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2.4718 Acc: 0.4021\n",
      "val Loss: 3.1538 Acc: 0.2594\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2.4015 Acc: 0.4242\n",
      "val Loss: 3.1498 Acc: 0.2594\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2.3925 Acc: 0.4229\n",
      "val Loss: 3.1278 Acc: 0.2802\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2.3526 Acc: 0.4394\n",
      "val Loss: 3.1105 Acc: 0.2694\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2.3539 Acc: 0.4436\n",
      "val Loss: 3.1271 Acc: 0.2736\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2.3249 Acc: 0.4325\n",
      "val Loss: 3.1129 Acc: 0.2836\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2.3099 Acc: 0.4467\n",
      "val Loss: 3.0815 Acc: 0.2736\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2.2786 Acc: 0.4526\n",
      "val Loss: 3.0984 Acc: 0.2677\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2.2590 Acc: 0.4578\n",
      "val Loss: 3.1139 Acc: 0.2786\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2.2399 Acc: 0.4678\n",
      "val Loss: 3.0916 Acc: 0.2861\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2.2370 Acc: 0.4632\n",
      "val Loss: 3.0770 Acc: 0.2911\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2.2508 Acc: 0.4653\n",
      "val Loss: 3.1032 Acc: 0.2836\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2.2101 Acc: 0.4692\n",
      "val Loss: 3.0620 Acc: 0.2877\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2.2058 Acc: 0.4659\n",
      "val Loss: 3.0982 Acc: 0.2869\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2.1668 Acc: 0.4780\n",
      "val Loss: 3.0654 Acc: 0.3044\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2.1890 Acc: 0.4724\n",
      "val Loss: 3.0737 Acc: 0.2944\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2.2010 Acc: 0.4732\n",
      "val Loss: 3.0759 Acc: 0.2919\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2.1378 Acc: 0.4870\n",
      "val Loss: 3.0526 Acc: 0.2961\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2.1577 Acc: 0.4786\n",
      "val Loss: 3.0531 Acc: 0.2944\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2.1162 Acc: 0.4922\n",
      "val Loss: 3.0553 Acc: 0.2902\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2.0723 Acc: 0.5055\n",
      "val Loss: 3.0620 Acc: 0.2919\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2.0848 Acc: 0.4961\n",
      "val Loss: 3.0565 Acc: 0.2861\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2.1034 Acc: 0.5039\n",
      "val Loss: 3.0969 Acc: 0.2994\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2.0548 Acc: 0.5093\n",
      "val Loss: 3.0500 Acc: 0.2977\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2.0281 Acc: 0.5228\n",
      "val Loss: 3.0227 Acc: 0.3003\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.9948 Acc: 0.5141\n",
      "val Loss: 2.9954 Acc: 0.3086\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.9735 Acc: 0.5339\n",
      "val Loss: 3.0015 Acc: 0.2994\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.9864 Acc: 0.5328\n",
      "val Loss: 2.9911 Acc: 0.3036\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2.0003 Acc: 0.5264\n",
      "val Loss: 2.9976 Acc: 0.3003\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.9868 Acc: 0.5324\n",
      "val Loss: 2.9890 Acc: 0.3069\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.9782 Acc: 0.5287\n",
      "val Loss: 2.9965 Acc: 0.3053\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.9482 Acc: 0.5429\n",
      "val Loss: 2.9870 Acc: 0.3053\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.9502 Acc: 0.5468\n",
      "val Loss: 3.0020 Acc: 0.3011\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.9867 Acc: 0.5306\n",
      "val Loss: 2.9970 Acc: 0.3061\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 1.9644 Acc: 0.5445\n",
      "val Loss: 3.0008 Acc: 0.3036\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.9603 Acc: 0.5331\n",
      "val Loss: 2.9881 Acc: 0.3061\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.9648 Acc: 0.5337\n",
      "val Loss: 2.9885 Acc: 0.3019\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.9677 Acc: 0.5485\n",
      "val Loss: 2.9926 Acc: 0.3078\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 1.9490 Acc: 0.5443\n",
      "val Loss: 2.9864 Acc: 0.3094\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 1.9687 Acc: 0.5424\n",
      "val Loss: 2.9967 Acc: 0.3078\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.9460 Acc: 0.5485\n",
      "val Loss: 3.0000 Acc: 0.3019\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 1.9598 Acc: 0.5339\n",
      "val Loss: 2.9845 Acc: 0.3161\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 1.9726 Acc: 0.5391\n",
      "val Loss: 2.9885 Acc: 0.3061\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 1.9365 Acc: 0.5472\n",
      "val Loss: 2.9857 Acc: 0.3094\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 1.9596 Acc: 0.5393\n",
      "val Loss: 2.9793 Acc: 0.3019\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 1.9371 Acc: 0.5495\n",
      "val Loss: 2.9933 Acc: 0.2994\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 1.9486 Acc: 0.5468\n",
      "val Loss: 2.9797 Acc: 0.3069\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 1.9405 Acc: 0.5449\n",
      "val Loss: 2.9802 Acc: 0.3069\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 1.9587 Acc: 0.5356\n",
      "val Loss: 2.9933 Acc: 0.3044\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 1.9455 Acc: 0.5443\n",
      "val Loss: 2.9840 Acc: 0.3061\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 1.9434 Acc: 0.5485\n",
      "val Loss: 2.9791 Acc: 0.3128\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 1.9608 Acc: 0.5399\n",
      "val Loss: 2.9812 Acc: 0.3128\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 1.9165 Acc: 0.5612\n",
      "val Loss: 2.9873 Acc: 0.3053\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 1.9066 Acc: 0.5575\n",
      "val Loss: 2.9824 Acc: 0.3094\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 1.9150 Acc: 0.5529\n",
      "val Loss: 2.9833 Acc: 0.3078\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 1.9401 Acc: 0.5502\n",
      "val Loss: 2.9768 Acc: 0.3053\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 1.9471 Acc: 0.5468\n",
      "val Loss: 2.9807 Acc: 0.3053\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 1.9724 Acc: 0.5343\n",
      "val Loss: 2.9860 Acc: 0.3086\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 1.9134 Acc: 0.5558\n",
      "val Loss: 2.9754 Acc: 0.3078\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 1.9191 Acc: 0.5499\n",
      "val Loss: 2.9980 Acc: 0.3078\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 1.9269 Acc: 0.5445\n",
      "val Loss: 2.9802 Acc: 0.3086\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 1.9055 Acc: 0.5579\n",
      "val Loss: 2.9813 Acc: 0.3036\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 1.9409 Acc: 0.5477\n",
      "val Loss: 2.9850 Acc: 0.3061\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 1.9391 Acc: 0.5510\n",
      "val Loss: 2.9807 Acc: 0.3011\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 1.9210 Acc: 0.5552\n",
      "val Loss: 2.9785 Acc: 0.3061\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 1.9481 Acc: 0.5472\n",
      "val Loss: 2.9847 Acc: 0.3061\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 1.9219 Acc: 0.5431\n",
      "val Loss: 2.9788 Acc: 0.3086\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 1.9531 Acc: 0.5391\n",
      "val Loss: 2.9932 Acc: 0.3044\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 1.9266 Acc: 0.5531\n",
      "val Loss: 2.9874 Acc: 0.3111\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 1.9302 Acc: 0.5468\n",
      "val Loss: 2.9847 Acc: 0.3136\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 1.9158 Acc: 0.5531\n",
      "val Loss: 2.9904 Acc: 0.3119\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 1.9491 Acc: 0.5414\n",
      "val Loss: 2.9944 Acc: 0.3061\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 1.9381 Acc: 0.5435\n",
      "val Loss: 2.9811 Acc: 0.3078\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 1.9331 Acc: 0.5447\n",
      "val Loss: 2.9856 Acc: 0.3069\n",
      "Best val Acc: 0.3161\n"
     ]
    }
   ],
   "source": [
    "# Train a model from scratch\n",
    "model_scratch = models.resnet18()  # no pre-trained weights used\n",
    "model_scratch.fc = nn.Linear(model_scratch.fc.in_features, 200)\n",
    "model_scratch = model_scratch.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model_scratch.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "model_scratch = train_model_scratch(model_scratch, dataloaders, dataset_sizes, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights\n",
    "model_ft_load = models.resnet18()\n",
    "num_ftrs = model_ft_load.fc.in_features\n",
    "model_ft_load.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft_load = model_ft_load.to(device)\n",
    "\n",
    "model_ft_load.load_state_dict(torch.load('CUB_best_weights_ft.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Model - Test Loss: 1.1380 Test Acc: 0.7325\n",
      "Scratch Model - Test Loss: 2.8258 Test Acc: 0.3243\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss_ft, test_acc_ft = evaluate_model(\n",
    "    model_ft_load, dataloaders, criterion, phase='test')\n",
    "test_loss_scratch, test_acc_scratch = evaluate_model(\n",
    "    model_scratch, dataloaders, criterion, phase='test')\n",
    "print(\n",
    "    f'Pre-trained Model - Test Loss: {test_loss_ft:.4f} Test Acc: {test_acc_ft:.4f}')\n",
    "print(\n",
    "    f'Scratch Model - Test Loss: {test_loss_scratch:.4f} Test Acc: {test_acc_scratch:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
