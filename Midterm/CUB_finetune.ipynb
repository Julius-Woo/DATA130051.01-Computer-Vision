{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Fine-tuning a CNN Pretrained on ImageNet for Bird Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelimiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set hyperparameters\n",
    "data_dir = 'CUB_200_2011'\n",
    "num_classes = 200\n",
    "batch_size = 64\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "l2 = 0.01\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load data\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=8) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, dataloaders, dataset_sizes, learning_rate, num_epochs, l2, patience, param_group=True):\n",
    "    \"\"\"\n",
    "    Fine-tunes a pretrained model on a new dataset.\n",
    "    \n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network to train.\n",
    "        dataloaders (dict): A dictionary containing the training and validation dataloaders.\n",
    "        dataset_sizes (dict): A dictionary containing the sizes of the training and validation datasets.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        num_epochs (int, optional): The number of epochs to train for.\n",
    "        l2: Weight decay strength.\n",
    "        patience: The patience number for early stopping.\n",
    "        param_group (bool, optional): If True, use a higher learning rate for the final layer. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "        net (torch.nn.Module): The fine-tuned neural network.\n",
    "    \"\"\"\n",
    "    log_dir = os.path.join(\"runs/CUB_bird_classification\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Define the optimizer with parameter groups if param_group is True\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters() if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        optimizer = optim.SGD([\n",
    "            {'params': params_1x},\n",
    "            {'params': net.fc.parameters(), 'lr': learning_rate * 10}\n",
    "        ], lr=learning_rate, weight_decay=l2, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=l2, momentum=momentum)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.05)\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())  # Save the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)  # Forward pass\n",
    "                    _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "                    loss = loss_fn(outputs, labels)  # Compute loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # Backward pass\n",
    "                        optimizer.step()  # Optimize the model\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # TensorBoard\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Training loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Training accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('Validation loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # Deep copy the model if the validation accuracy is the best seen so far\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                # Early stopping\n",
    "                if epochs_no_improve == patience:\n",
    "                    print('Early stopping!')\n",
    "                    net.load_state_dict(best_model_wts)\n",
    "                    writer.close()\n",
    "                    torch.save(net.state_dict(), 'CUB_best_weights_ft.pth')\n",
    "                    return net\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    torch.save(net.state_dict(), 'CUB_best_weights_ft.pth')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning_search(net, dataloaders, dataset_sizes, lr_small, lr_large, num_epochs, l2, param_group=True):\n",
    "    \"\"\"\n",
    "    Function for grid search, verbose outputs omitted.\n",
    "    \"\"\"\n",
    "    log_dir = os.path.join(\"runs/CUB_bird_search\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define the optimizer with parameter groups if param_group is True\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters() if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        optimizer = optim.SGD([\n",
    "            {'params': params_1x},\n",
    "            {'params': net.fc.parameters(), 'lr': lr_large}\n",
    "        ], lr=lr_small, weight_decay=l2, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr_small, weight_decay=l2, momentum=momentum)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())  # Save the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)  # Forward pass\n",
    "                    _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "                    loss = loss_fn(outputs, labels)  # Compute loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # Backward pass\n",
    "                        optimizer.step()  # Optimize the model\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            if epoch == num_epochs - 1:\n",
    "                final_loss = epoch_loss\n",
    "                final_acc = epoch_acc\n",
    "                print(f'Final {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # TensorBoard\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Training loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Training accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('Validation loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Validation accuracy', epoch_acc, epoch)\n",
    "            \n",
    "            # Deep copy the model if the validation accuracy is the best seen so far\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return net, final_loss, final_acc, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model from random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_scratch(net, dataloaders, dataset_sizes, optimizer, num_epochs):\n",
    "    \"\"\"Function for training a model from scratch.\n",
    "    \"\"\"\n",
    "    log_dir = os.path.join(\"runs/CUB_from_scratch\",\n",
    "                           datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.05)\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('training loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('training accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('validation loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('validation accuracy', epoch_acc, epoch)\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    torch.save(net.state_dict(), 'CUB_best_weights_scratch.pth')\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloaders, criterion, phase='test'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    loss = running_loss / dataset_sizes[phase]\n",
    "    acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "    # print(f'{phase} Loss: {loss:.4f} Acc: {acc:.4f}')\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best parameters\n",
    "results = []\n",
    "\n",
    "def search_param(lrs, epochs, l2_s):\n",
    "    for lr in lrs:\n",
    "        for epoch in epochs:\n",
    "            for l2 in l2_s:\n",
    "                print(f'----Training with lr={lr}, epochs={epoch}, l2={l2}----')\n",
    "                net = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "                net.fc = nn.Linear(net.fc.in_features, 200)\n",
    "                net = net.to(device)\n",
    "\n",
    "                trained_net, final_loss, final_acc, best_acc = train_fine_tuning_search(\n",
    "                    net, dataloaders, dataset_sizes, lr[0], lr[1], epoch, l2)\n",
    "\n",
    "                test_loss, test_acc = evaluate_model(trained_net, dataloaders, nn.CrossEntropyLoss())\n",
    "\n",
    "                results.append((lr, epoch, l2, final_loss, final_acc, test_loss, test_acc, best_acc))\n",
    "                print(f'Test loss: {test_loss:.4f} Acc: {test_acc:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training with lr=(0.01, 0.1), epochs=15, l2=0.01----\n",
      "Final train Loss: 2.1697 Acc: 0.5078\n",
      "Final val Loss: 2.4523 Acc: 0.4178\n",
      "Best val Acc: 0.4904\n",
      "Test loss: 1.9993 Acc: 0.5074\n",
      "\n",
      "----Training with lr=(0.01, 0.01), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.9356 Acc: 0.5969\n",
      "Final val Loss: 2.1083 Acc: 0.4879\n",
      "Best val Acc: 0.5596\n",
      "Test loss: 1.7709 Acc: 0.5690\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.2869 Acc: 0.7358\n",
      "Final val Loss: 1.2314 Acc: 0.7048\n",
      "Best val Acc: 0.7106\n",
      "Test loss: 1.1907 Acc: 0.7164\n",
      "\n",
      "----Training with lr=(0.001, 0.001), epochs=15, l2=0.01----\n",
      "Final train Loss: 2.3232 Acc: 0.5994\n",
      "Final val Loss: 2.0525 Acc: 0.5997\n",
      "Best val Acc: 0.5997\n",
      "Test loss: 1.9915 Acc: 0.6134\n",
      "\n",
      "----Training with lr=(0.0005, 0.005), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.5252 Acc: 0.6970\n",
      "Final val Loss: 1.4174 Acc: 0.6747\n",
      "Best val Acc: 0.6747\n",
      "Test loss: 1.3381 Acc: 0.6964\n",
      "\n",
      "----Training with lr=(0.0001, 0.001), epochs=15, l2=0.01----\n",
      "Final train Loss: 3.1223 Acc: 0.4655\n",
      "Final val Loss: 2.8362 Acc: 0.4762\n",
      "Best val Acc: 0.4762\n",
      "Test loss: 2.7930 Acc: 0.4926\n",
      "\n",
      "----Training with lr=(1e-05, 0.0001), epochs=15, l2=0.01----\n",
      "Final train Loss: 5.0962 Acc: 0.0265\n",
      "Final val Loss: 5.0743 Acc: 0.0242\n",
      "Best val Acc: 0.0250\n",
      "Test loss: 5.1137 Acc: 0.0250\n",
      "\n",
      "----Training with lr=(5e-05, 0.0005), epochs=15, l2=0.01----\n",
      "Final train Loss: 4.1048 Acc: 0.2776\n",
      "Final val Loss: 3.9321 Acc: 0.2702\n",
      "Best val Acc: 0.2702\n",
      "Test loss: 3.9168 Acc: 0.2922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for lrs and epochs\n",
    "# (small, large)\n",
    "lrs = [(0.01, 0.1), (0.01, 0.01), (0.001, 0.01), (0.001, 0.001), (5e-4, 5e-3), \n",
    "       (1e-4, 1e-3), (1e-5, 1e-4), (5e-5, 5e-4)]\n",
    "epoch = 15\n",
    "\n",
    "search_param(lrs, [epoch], [0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training with lr=(0.001, 0.01), epochs=15, l2=0.01----\n",
      "Final train Loss: 1.2756 Acc: 0.7426\n",
      "Final val Loss: 1.2268 Acc: 0.7056\n",
      "Best val Acc: 0.7056\n",
      "Test loss: 1.1648 Acc: 0.7194\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.01----\n",
      "Final train Loss: 1.0975 Acc: 0.8017\n",
      "Final val Loss: 1.1870 Acc: 0.7248\n",
      "Best val Acc: 0.7248\n",
      "Test loss: 1.1144 Acc: 0.7347\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=45, l2=0.01----\n",
      "Final train Loss: 0.9513 Acc: 0.8367\n",
      "Final val Loss: 1.1712 Acc: 0.7206\n",
      "Best val Acc: 0.7381\n",
      "Test loss: 1.1074 Acc: 0.7453\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=65, l2=0.01----\n",
      "Final train Loss: 0.8487 Acc: 0.8659\n",
      "Final val Loss: 1.1642 Acc: 0.7323\n",
      "Best val Acc: 0.7456\n",
      "Test loss: 1.0980 Acc: 0.7503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for epochs\n",
    "lr = (0.001, 0.01)\n",
    "epochs = [15, 25, 45, 65]\n",
    "l2 = 0.01\n",
    "search_param([lr], epochs, [l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.1----\n",
      "Final train Loss: 4.4820 Acc: 0.1007\n",
      "Final val Loss: 4.3680 Acc: 0.1243\n",
      "Best val Acc: 0.3770\n",
      "Test loss: 3.2471 Acc: 0.3907\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.01----\n",
      "Final train Loss: 1.1049 Acc: 0.7931\n",
      "Final val Loss: 1.1633 Acc: 0.7189\n",
      "Best val Acc: 0.7256\n",
      "Test loss: 1.1167 Acc: 0.7301\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.001----\n",
      "Final train Loss: 0.6861 Acc: 0.8482\n",
      "Final val Loss: 1.0361 Acc: 0.7198\n",
      "Best val Acc: 0.7248\n",
      "Test loss: 0.9837 Acc: 0.7275\n",
      "\n",
      "----Training with lr=(0.001, 0.01), epochs=25, l2=0.0001----\n",
      "Final train Loss: 0.6500 Acc: 0.8515\n",
      "Final val Loss: 1.0420 Acc: 0.7198\n",
      "Best val Acc: 0.7206\n",
      "Test loss: 0.9797 Acc: 0.7287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for l2 regularization strengths\n",
    "l2_s = [0.1, 0.01, 0.001, 0.0001]\n",
    "epoch = 25\n",
    "lr = (0.001, 0.01)\n",
    "search_param([lr], [epoch], l2_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 5.5233 Acc: 0.0156\n",
      "val Loss: 4.7857 Acc: 0.0601\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 4.3770 Acc: 0.1255\n",
      "val Loss: 3.7077 Acc: 0.1977\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 3.5087 Acc: 0.2609\n",
      "val Loss: 2.9357 Acc: 0.3403\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 2.8977 Acc: 0.3973\n",
      "val Loss: 2.4918 Acc: 0.4170\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.4901 Acc: 0.4792\n",
      "val Loss: 2.2088 Acc: 0.5071\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 2.2320 Acc: 0.5437\n",
      "val Loss: 2.0041 Acc: 0.5388\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.0737 Acc: 0.5704\n",
      "val Loss: 1.8437 Acc: 0.5680\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.9085 Acc: 0.6148\n",
      "val Loss: 1.7391 Acc: 0.5955\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.8279 Acc: 0.6265\n",
      "val Loss: 1.6664 Acc: 0.6155\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.7025 Acc: 0.6542\n",
      "val Loss: 1.6126 Acc: 0.6163\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.6307 Acc: 0.6753\n",
      "val Loss: 1.5453 Acc: 0.6480\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.5866 Acc: 0.6792\n",
      "val Loss: 1.4966 Acc: 0.6497\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.5517 Acc: 0.6872\n",
      "val Loss: 1.4714 Acc: 0.6539\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.4666 Acc: 0.7034\n",
      "val Loss: 1.4355 Acc: 0.6739\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.4578 Acc: 0.7016\n",
      "val Loss: 1.4111 Acc: 0.6606\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.4015 Acc: 0.7358\n",
      "val Loss: 1.3822 Acc: 0.6706\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.3731 Acc: 0.7314\n",
      "val Loss: 1.3731 Acc: 0.6697\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.3685 Acc: 0.7391\n",
      "val Loss: 1.3506 Acc: 0.6814\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.3125 Acc: 0.7460\n",
      "val Loss: 1.3344 Acc: 0.6906\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.3063 Acc: 0.7547\n",
      "val Loss: 1.3174 Acc: 0.6922\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2532 Acc: 0.7662\n",
      "val Loss: 1.2982 Acc: 0.6872\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2769 Acc: 0.7502\n",
      "val Loss: 1.2914 Acc: 0.6922\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2421 Acc: 0.7693\n",
      "val Loss: 1.2866 Acc: 0.6964\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2077 Acc: 0.7712\n",
      "val Loss: 1.2791 Acc: 0.6922\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2242 Acc: 0.7708\n",
      "val Loss: 1.2771 Acc: 0.6914\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.1867 Acc: 0.7831\n",
      "val Loss: 1.2607 Acc: 0.7023\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.1980 Acc: 0.7789\n",
      "val Loss: 1.2630 Acc: 0.7056\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.1748 Acc: 0.7864\n",
      "val Loss: 1.2519 Acc: 0.7114\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.1435 Acc: 0.7879\n",
      "val Loss: 1.2461 Acc: 0.7048\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.1749 Acc: 0.7798\n",
      "val Loss: 1.2299 Acc: 0.7123\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.1243 Acc: 0.7979\n",
      "val Loss: 1.2231 Acc: 0.7156\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.1134 Acc: 0.7987\n",
      "val Loss: 1.2406 Acc: 0.7064\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.1329 Acc: 0.7927\n",
      "val Loss: 1.2294 Acc: 0.7206\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.1083 Acc: 0.8033\n",
      "val Loss: 1.2265 Acc: 0.7189\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.1123 Acc: 0.8013\n",
      "val Loss: 1.2200 Acc: 0.7148\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.0792 Acc: 0.8092\n",
      "val Loss: 1.2156 Acc: 0.7106\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.0712 Acc: 0.8102\n",
      "val Loss: 1.2052 Acc: 0.7339\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.0857 Acc: 0.8117\n",
      "val Loss: 1.2041 Acc: 0.7114\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.0767 Acc: 0.8052\n",
      "val Loss: 1.1960 Acc: 0.7223\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.0634 Acc: 0.8092\n",
      "val Loss: 1.2061 Acc: 0.7181\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.0535 Acc: 0.8138\n",
      "val Loss: 1.1956 Acc: 0.7248\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.0495 Acc: 0.8192\n",
      "val Loss: 1.2032 Acc: 0.7248\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet-18 model\n",
    "model_ft = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "# Modify the fully connected layer to match the number of output classes\n",
    "model_ft.fc = nn.Linear(model_ft.fc.in_features, num_classes)\n",
    "# Initialize the weights of FC layer using Xavier uniform initialization\n",
    "nn.init.xavier_uniform_(model_ft.fc.weight)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "model_ft = train_fine_tuning(model_ft, dataloaders, dataset_sizes, learning_rate, num_epochs, l2, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 5.3429 Acc: 0.0060\n",
      "val Loss: 5.2505 Acc: 0.0150\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 5.1411 Acc: 0.0152\n",
      "val Loss: 5.0160 Acc: 0.0250\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.9315 Acc: 0.0284\n",
      "val Loss: 4.8787 Acc: 0.0259\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 4.7658 Acc: 0.0352\n",
      "val Loss: 4.7743 Acc: 0.0300\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 4.6545 Acc: 0.0459\n",
      "val Loss: 4.7240 Acc: 0.0475\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 4.5655 Acc: 0.0503\n",
      "val Loss: 4.6486 Acc: 0.0500\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 4.4722 Acc: 0.0624\n",
      "val Loss: 4.6186 Acc: 0.0500\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 4.4153 Acc: 0.0644\n",
      "val Loss: 4.5389 Acc: 0.0701\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 4.3336 Acc: 0.0784\n",
      "val Loss: 4.5826 Acc: 0.0492\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 4.2915 Acc: 0.0822\n",
      "val Loss: 4.4854 Acc: 0.0684\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 4.2035 Acc: 0.0897\n",
      "val Loss: 4.4714 Acc: 0.0601\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 4.1619 Acc: 0.0938\n",
      "val Loss: 4.2928 Acc: 0.0842\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 4.1015 Acc: 0.0984\n",
      "val Loss: 4.5231 Acc: 0.0726\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 4.0731 Acc: 0.1059\n",
      "val Loss: 4.3382 Acc: 0.0692\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 3.9986 Acc: 0.1168\n",
      "val Loss: 4.2366 Acc: 0.0976\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 3.9533 Acc: 0.1281\n",
      "val Loss: 4.2298 Acc: 0.0759\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 3.8832 Acc: 0.1356\n",
      "val Loss: 4.1347 Acc: 0.1009\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 3.8482 Acc: 0.1416\n",
      "val Loss: 4.2358 Acc: 0.0801\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 3.8195 Acc: 0.1379\n",
      "val Loss: 4.0756 Acc: 0.1009\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 3.7884 Acc: 0.1531\n",
      "val Loss: 4.0962 Acc: 0.1051\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 3.7525 Acc: 0.1522\n",
      "val Loss: 4.3028 Acc: 0.0817\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 3.7252 Acc: 0.1631\n",
      "val Loss: 3.9637 Acc: 0.1126\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 3.7047 Acc: 0.1681\n",
      "val Loss: 4.1197 Acc: 0.1018\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 3.6365 Acc: 0.1810\n",
      "val Loss: 4.0052 Acc: 0.1218\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 3.5822 Acc: 0.1869\n",
      "val Loss: 4.1548 Acc: 0.1034\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 3.5605 Acc: 0.1869\n",
      "val Loss: 3.9767 Acc: 0.1218\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 3.5558 Acc: 0.1925\n",
      "val Loss: 4.3091 Acc: 0.0826\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 3.5342 Acc: 0.1992\n",
      "val Loss: 3.7709 Acc: 0.1576\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 3.4999 Acc: 0.2040\n",
      "val Loss: 3.7997 Acc: 0.1635\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 3.4859 Acc: 0.2035\n",
      "val Loss: 3.8995 Acc: 0.1410\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 3.4582 Acc: 0.2106\n",
      "val Loss: 3.9466 Acc: 0.1251\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 3.4451 Acc: 0.2186\n",
      "val Loss: 3.9121 Acc: 0.1418\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 3.4108 Acc: 0.2181\n",
      "val Loss: 3.7760 Acc: 0.1243\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 3.4264 Acc: 0.2246\n",
      "val Loss: 3.7031 Acc: 0.1610\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 3.3987 Acc: 0.2225\n",
      "val Loss: 3.8698 Acc: 0.1535\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 3.3638 Acc: 0.2330\n",
      "val Loss: 3.6781 Acc: 0.1526\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 3.3135 Acc: 0.2446\n",
      "val Loss: 3.7079 Acc: 0.1676\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 3.3228 Acc: 0.2411\n",
      "val Loss: 4.3717 Acc: 0.0942\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 3.3256 Acc: 0.2413\n",
      "val Loss: 3.7709 Acc: 0.1593\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 3.2980 Acc: 0.2453\n",
      "val Loss: 3.7842 Acc: 0.1601\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 3.2570 Acc: 0.2484\n",
      "val Loss: 4.3113 Acc: 0.0992\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 3.2358 Acc: 0.2632\n",
      "val Loss: 3.5728 Acc: 0.1751\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 3.2479 Acc: 0.2565\n",
      "val Loss: 3.5640 Acc: 0.1676\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 3.2374 Acc: 0.2588\n",
      "val Loss: 3.6278 Acc: 0.1618\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 3.2098 Acc: 0.2722\n",
      "val Loss: 3.6399 Acc: 0.1635\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 3.1832 Acc: 0.2711\n",
      "val Loss: 3.5472 Acc: 0.1793\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 3.1291 Acc: 0.2901\n",
      "val Loss: 3.6416 Acc: 0.1701\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 3.0892 Acc: 0.2976\n",
      "val Loss: 3.3569 Acc: 0.2168\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 3.1101 Acc: 0.2818\n",
      "val Loss: 3.6464 Acc: 0.1626\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 3.0864 Acc: 0.2945\n",
      "val Loss: 3.7331 Acc: 0.1576\n",
      "\n",
      "Best val Acc: 0.2168\n"
     ]
    }
   ],
   "source": [
    "# Train a model from scratch\n",
    "model_scratch = models.resnet18()  # no pre-trained weights used\n",
    "model_scratch.fc = nn.Linear(model_scratch.fc.in_features, 200)\n",
    "model_scratch = model_scratch.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model_scratch.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "model_scratch = train_model_scratch(model_scratch, dataloaders, dataset_sizes, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights\n",
    "model_ft_load = models.resnet18()\n",
    "num_ftrs = model_ft_load.fc.in_features\n",
    "model_ft_load.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft_load = model_ft_load.to(device)\n",
    "\n",
    "model_ft_load.load_state_dict(torch.load('CUB_best_weights_ft.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Model - Test Loss: 1.1380 Test Acc: 0.7325\n",
      "Scratch Model - Test Loss: 3.2839 Test Acc: 0.2287\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss_ft, test_acc_ft = evaluate_model(\n",
    "    model_ft, dataloaders, criterion, phase='test')\n",
    "test_loss_scratch, test_acc_scratch = evaluate_model(\n",
    "    model_scratch, dataloaders, criterion, phase='test')\n",
    "print(\n",
    "    f'Pre-trained Model - Test Loss: {test_loss_ft:.4f} Test Acc: {test_acc_ft:.4f}')\n",
    "print(\n",
    "    f'Scratch Model - Test Loss: {test_loss_scratch:.4f} Test Acc: {test_acc_scratch:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
